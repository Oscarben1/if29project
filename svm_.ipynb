{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook projet IF29\n",
    "\n",
    "Ce Notebook fait office de rendu de code pour le projet d'IF29. Le but étant, à partir d'une base de données de plus de 4 millions de tweets, de réaliser un algorithme supervisé et un non-supervisé afin de classifier les utilisateurs comme suspect ou non.\n",
    "\n",
    "## Importation des librairies utiles au projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données dans un DF Pandas\n",
    "On récupère les données parsées dans la base SQLite que l'on a exporté dans un .csv puis on enlève les colonnes identifiant et identifiant_user qui ne nous seront pas utiles pour notre analyse. On choisi de ne prendre que 400000 users pour nos traitements car nos machines ne supportent pas le traitement de toute la base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/finalDF.csv').drop([\"id\", \"id_user\"], axis=1).iloc[500000:900000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrage-réduction des données pour réaliser l'ACP\n",
    "On centre réduit les données pour supprimer la variabilité des données à cause de leur unité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On centre réduit les données\n",
    "s_sc = StandardScaler() \n",
    "df_processed = s_sc.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réalisation de l'ACP\n",
    "On réalise l'ACP pour réduire la dimensionnalité de notre dataframe, afin de réaliser l'algorithme non-supervisé K-Means. On passe donc de 8 variables à 2 variables qui sont les composantes principales retenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On réalise l'ACP\n",
    "modelPCA = PCA(n_components=2)\n",
    "df_reduced = modelPCA.fit_transform(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère ici les variances expliquées par chaque composante principale afin d'avoir cette indication sur notre graph. On trace donc le graph avec les deux composantes principales comme axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP1inertie = str(round(round(modelPCA.explained_variance_ratio_[0],3)*100,1))\n",
    "CP2inertie = str(round(round(modelPCA.explained_variance_ratio_[1],3)*100,1))\n",
    "xlab = str(\"CP1 (\"+CP1inertie+\"%)\")\n",
    "ylab = str(\"CP2 (\"+CP2inertie+\"%)\")\n",
    "plt.scatter(df_reduced[:,0],df_reduced[:,1])\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel(ylab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouver le nombre de clusters pour le K-Means\n",
    "Pour trouver le nombre de clusters que nous allons prendre pour le K-Means, on réalise la méthode du coude représentant l'inertie en fonction du nombre de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method\n",
    "inertia = []\n",
    "K_range = range(1, 8)\n",
    "for i in K_range:\n",
    "    modelElbow = KMeans(n_clusters=i).fit(df_reduced)\n",
    "    inertia.append(modelElbow.inertia_)\n",
    "\n",
    "plt.plot(K_range, inertia)\n",
    "plt.xlabel('nb de clusters')\n",
    "plt.ylabel('Inertie')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means avec 3 clusters\n",
    "On réalise le K-Means avec 3 clusters, car c'est notre zone de coude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans with 3 clusters\n",
    "modelKMeans = KMeans(n_clusters=3)\n",
    "df_KMeans = modelKMeans.fit(df_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trace le graph en donnant une couleur définie à nos clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = modelKMeans.fit_predict(df_reduced)\n",
    "\n",
    "filtered_label0 = df_reduced[label == 0]\n",
    "filtered_label1 = df_reduced[label == 1]\n",
    "filtered_label2 = df_reduced[label == 2]\n",
    "plt.scatter(filtered_label0[:,0] , filtered_label0[:,1] , color = 'red')\n",
    "plt.scatter(filtered_label1[:,0] , filtered_label1[:,1] , color = 'black')\n",
    "plt.scatter(filtered_label2[:,0] , filtered_label2[:,1] , color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means avec 6 clusters\n",
    "Pour plus de représentativité par rapport à la problématique, on réalise un K-Means avec 6 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans with 6 clusters\n",
    "modelKMeans = KMeans(n_clusters=6)\n",
    "df_KMeans = modelKMeans.fit(df_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trace le graph avec les 6 clusters en choisissant la couleur de chacun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = modelKMeans.fit_predict(df_reduced)\n",
    "\n",
    "filtered_label0 = df_reduced[label == 0]\n",
    "filtered_label1 = df_reduced[label == 1]\n",
    "filtered_label2 = df_reduced[label == 2]\n",
    "filtered_label3 = df_reduced[label == 3]\n",
    "filtered_label4 = df_reduced[label == 4]\n",
    "filtered_label5 = df_reduced[label == 5]\n",
    "plt.scatter(filtered_label0[:,0] , filtered_label0[:,1] , color = 'red')\n",
    "plt.scatter(filtered_label1[:,0] , filtered_label1[:,1] , color = 'black')\n",
    "plt.scatter(filtered_label2[:,0] , filtered_label2[:,1] , color = 'green')\n",
    "plt.scatter(filtered_label3[:,0] , filtered_label3[:,1] , color = 'cyan')\n",
    "plt.scatter(filtered_label4[:,0] , filtered_label4[:,1] , color = 'magenta')\n",
    "plt.scatter(filtered_label5[:,0] , filtered_label5[:,1] , color = 'yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labellisation de nos utilisateurs\n",
    "On labellise les utilisateurs comme suspect ou non en fonction du résultat du K-Means. Le cluster ayant le pourcentage de comptes supprimés le plus élevé est considéré comme étant suspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User from each clusters\n",
    "\n",
    "cluster0 = pd.DataFrame(df-reduced[df_KMeans.labels_==0])\n",
    "cluster1 = pd.DataFrame(df-reduced[df_KMeans.labels_==1])\n",
    "cluster2 = pd.DataFrame(df_reduced[df_KMeans.labels_==2])\n",
    "cluster3 = pd.DataFrame(df_reduced[df_KMeans.labels_==3])\n",
    "cluster4 = pd.DataFrame(df_reduced[df_KMeans.labels_==4])\n",
    "cluster5 = pd.DataFrame(df_reduced[df_KMeans.labels_==5])\n",
    "\n",
    "cluster0['suspect'] = 0\n",
    "cluster1['suspect'] = 0\n",
    "cluster2['suspect'] = 0\n",
    "cluster3['suspect'] = 1\n",
    "cluster4['suspect'] = 0\n",
    "cluster5['suspect'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " On concatène nos 6 clusters pour creer un dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label = pd.concat([cluster0, cluster1, cluster2, cluster3, cluster4, cluster5])\n",
    "dataset_label.to_csv('data/dataset_label.csv', encoding='utf-8')\n",
    "dataset_final = np.array(dataset_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "## Séparation des labels\n",
    "On sépare les variables X de leur label Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_final[:,:-1]\n",
    "Y = dataset_final[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répartition Training/Test\n",
    "On sépare notre jeu de données en 2 sous-datasets, un pour l'entrainement du model et un pour le test, comprenant respectivement 80% et 20% des données prises aléatoirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du model\n",
    "On entraine tout d'abord le model avec la fonction noyau 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b229c6d0fd4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "linear = svm.SVC(kernel='linear')\n",
    "\n",
    "linear.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test et analyse\n",
    "On test le model sur nos données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On analyse les résultats avec le dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], c = Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dfdc9f200e3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                   (\"Matrice de confusion normalisée\", 'true')]\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtitles_options\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     disp = plot_confusion_matrix(linear, X_test, Y_test,\n\u001b[0m\u001b[0;32m      7\u001b[0m                                  \u001b[0mdisplay_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'suspect'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                  \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBlues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'linear' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Matrice de confusion, avec normalisation\", None),\n",
    "                  (\"Matrice de confusion normalisée\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(linear, X_test, Y_test,\n",
    "                                 display_labels=['normal','suspect'],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM avec la fonction poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = svm.SVC(kernel='poly')\n",
    "\n",
    "poly.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = poly.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], c = Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM avec la fonction rdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = svm.SVC(kernel='rbf')\n",
    "\n",
    "rbf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], c = Y_pred)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84a072aa058ce2957057505d5ffd0b64ceae390b849ae71bc8d63a6b25d09926"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
