{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook projet IF29\n",
    "\n",
    "Ce Notebook fait office de rendu de code pour le projet d'IF29. Le but étant, à partir d'une base de données de plus de 4 millions de tweets, de réaliser un algorithme supervisé et un non-supervisé afin de classifier les utilisateurs comme suspect ou non.\n",
    "\n",
    "## Importation des librairies utiles au projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    "    Float,\n",
    ")\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation de la base de données JSON\n",
    "Pour commencer, nous avons parsé les fichiers JSON et récupéré 10 variables par tweets qui nous intéressaient et nous les avons inséré dans une base de donnée SQL via un ORM SQLAlchemy. L'ORM était composé de 2 tables : une pour les users et une pour les tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///Tweets.db\", connect_args={\"check_same_thread\": False})\n",
    "Base = declarative_base(bind=engine)\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"user\"\n",
    "\n",
    "    # Tables des users\n",
    "    id = Column(\"id\", Integer, primary_key=True)\n",
    "    id_user = Column(\"id_user\", Integer)\n",
    "    followers_count = Column(\"followers_count\", Integer)\n",
    "    friends_count = Column(\"friends_count\", Integer)\n",
    "    ratio = Column(\"ratio\", Float)\n",
    "    tweet_length = Column(\"tweet_length\", Float)\n",
    "    nombre_moyen_url = Column(\"nombre_moyen_url\", Float)\n",
    "    nombre_moyen_hashtag = Column(\"nombre_moyen_hashtag\", Float)\n",
    "    fav = Column(\"fav\", Integer)\n",
    "    agressivite = Column(\"agressivite\", Float)\n",
    "    visibilite = Column(\"visibilite\", Float)\n",
    "\n",
    "\n",
    "class Tweet(Base):\n",
    "    __tablename__ = \"tweet\"\n",
    "\n",
    "    # Tables des tweets\n",
    "    id = Column(\"id\", Integer, primary_key=True)\n",
    "    id_user = Column(\"id_user\", Integer)\n",
    "    name_user = Column(\"name_user\", String(500))\n",
    "    friends_count = Column(\"friends_count\", Integer)\n",
    "    followers_count = Column(\"followers_count\", Integer)\n",
    "    text = Column(\"text\", String(500))\n",
    "    hashtags = Column(\"hashtags\", String(500))\n",
    "    urls = Column(\"urls\", String(1000))\n",
    "    fav = Column(\"fav\", Integer)\n",
    "    user_mentions = Column(\"user_mentions\", String(500))\n",
    "    timestamp = Column(\"timestamp\", Integer)\n",
    "\n",
    "\n",
    "Base.metadata.create_all()\n",
    "session = Session()\n",
    "if __name__ == \"__main__\":\n",
    "    print(session)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois cette base de données crée, nous avons commencé à la remplir en extrayant JSON par JSON et tweet par tweet les variables qui nous intéressaient et en les stockant dans la db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction permettant de parser l'intégralité des json et de les stocker dans une base de donnée SQlite\n",
    "def get_tweet_from_json():\n",
    "    #boucle pour parcourir les fichier json 1 à 1\n",
    "    for i in range(0,2286):\n",
    "        #boucle pour parcourir les lignes et en extraire les tweets\n",
    "        for line in open('raw'+ str(i) +'.json', 'r', encoding='utf8'):\n",
    "            tweet_en_cours=json.loads(line)\n",
    "            id_user = int(tweet_en_cours[\"user\"]['id'])\n",
    "            name_user = tweet_en_cours[\"user\"]['name']\n",
    "            friends_count = int(tweet_en_cours[\"user\"]['friends_count'])\n",
    "            followers_count = int(tweet_en_cours[\"user\"]['followers_count'])\n",
    "            text = tweet_en_cours[\"text\"]\n",
    "            #Parcours de la liste des hashtags et concaténation en 1 string\n",
    "            hashtags = \"\"\n",
    "            for item in (tweet_en_cours[\"entities\"][\"hashtags\"]):\n",
    "                if hashtags == \"\":\n",
    "                    hashtags = item['text']\n",
    "                else :\n",
    "                    hashtags+=\", \" + item['text']\n",
    "            # Parcours de la liste des urls et concaténation en 1 string\n",
    "            urls = \"\"\n",
    "            for item in (tweet_en_cours[\"entities\"][\"urls\"]):\n",
    "\n",
    "                if urls == \"\":\n",
    "                    urls = item[\"url\"]\n",
    "                else:\n",
    "                    urls += \", \" + item[\"url\"]\n",
    "            user_mentions = len(tweet_en_cours[\"entities\"][\"user_mentions\"])\n",
    "            fav = int(tweet_en_cours[\"user\"]['favourites_count'])\n",
    "            timestamp= tweet_en_cours[\"timestamp_ms\"]\n",
    "            #stockage dans la db SQlite via l'ORM SQLAlchemy\n",
    "            try :\n",
    "\n",
    "                tweet = ORM.Tweet(\n",
    "                    id_user=id_user,\n",
    "                    name_user=name_user,\n",
    "                    friends_count=friends_count,\n",
    "                    followers_count=followers_count,\n",
    "                    text=text,\n",
    "                    hashtags=hashtags,\n",
    "                    urls=urls,\n",
    "                    fav=fav,\n",
    "                    user_mentions=user_mentions,\n",
    "                    timestamp=timestamp,\n",
    "                )\n",
    "                ORM.session.add(tweet)\n",
    "                ORM.session.commit()\n",
    "                ORM.session.flush()\n",
    "                ORM.session.refresh(tweet)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"erreur\")\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons donc récupéré 4.5 millions de tweets dans notre db SQLITE.\n",
    "\n",
    "## Transformation en users\n",
    "On travaille ensuite sur cette db pour en extraire les utilisateurs. Pour raccourcir le temps de traitement, on import dans un premier temps l'ensemble des tweets dans une liste de dictionnaire et on la trie par id_user (id des utilisateurs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweet():\n",
    "    tab_tweet=[]\n",
    "    # compteur de tweets utilisés\n",
    "    compteur=0\n",
    "    #on récupère les données tweets par tweets\n",
    "    for id, id_user, text, followers_count, friends_count, fav, hashtags, user_mentions, urls, timestamp in ORM.session.query(ORM.Tweet.id, ORM.Tweet.id_user, ORM.Tweet.text, ORM.Tweet.followers_count, ORM.Tweet.friends_count, ORM.Tweet.fav, ORM.Tweet.hashtags, ORM.Tweet.user_mentions, ORM.Tweet.urls, ORM.Tweet.timestamp).all():\n",
    "        tab_tweet.append({\"id\":id, \"id_user\":id_user, \"text\":text, \"followers_count\":followers_count, \"friends_count\":friends_count, \"fav\":fav, \"hashtags\":hashtags, \"user_mentions\":user_mentions, \"urls\":urls, \"timestamp\":timestamp})\n",
    "        compteur+=1\n",
    "        #on affiche le compteurs tous les 10000\n",
    "        if compteur % 100000 ==0:\n",
    "            print(compteur)\n",
    "    #on trie la liste par id_user\n",
    "    tab_tweet.sort(key=operator.itemgetter('id_user'))\n",
    "    print(\"tri fini\")\n",
    "    #Retourne la liste triée\n",
    "    return tab_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On donne ensuite la liste triée à la fonction chargée de créer les utilisateurs qui va parcourir les tweets et créer une nouvel utilisateur chaque fois qu'elle rencontre un nouvel id_user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Permet de créer la base de données des users\n",
    "\n",
    "Arg : tab : tableau des tweets triés par id_user\n",
    "Return None\n",
    "\"\"\"\n",
    "def create_user_table(tab):\n",
    "    #compteur de tweets utilisés\n",
    "    compteur=0\n",
    "    #initialisation id de l'utilisateur traité à l'étape d'avant\n",
    "    current_id = 0\n",
    "    #parcours des tweets de la liste\n",
    "    for rows in tab:\n",
    "        #Les tweets sont triés par id_user, on regarde donc si le tweet précédent à le même id_user pour le traiter ou le passer\n",
    "        if current_id==rows['id_user']:\n",
    "            #le user est déja traité, on ajoute 1 au compteur\n",
    "            compteur+=1\n",
    "            #on passe au tweet suivant\n",
    "            pass\n",
    "\n",
    "        else :\n",
    "            # le user n'est déja traité, on le crée et on stocke son user_id dans le comparateur current_id\n",
    "            current_id=rows['id_user']\n",
    "            #on vérifie que son friends_count est >0 pour éviter de diviser par 0\n",
    "            if (rows['friends_count'] != 0):\n",
    "                indicateur_ratio = rows['followers_count'] / rows['friends_count']\n",
    "            else:\n",
    "                indicateur_ratio = rows['followers_count']\n",
    "            liste_urls = \"\"\n",
    "            liste_hashtags = \"\"\n",
    "            nombre_user_mentions = 0\n",
    "            liste_tweets_utilisateur=[]\n",
    "            liste_tweets_utilisateur.append(rows)\n",
    "            total_text=0\n",
    "            compteur_recherche=compteur+1\n",
    "            #on récupère tous les tweets du même user\n",
    "            while rows['id_user']==tab[compteur_recherche]['id_user']:\n",
    "                liste_tweets_utilisateur.append(tab[compteur_recherche])\n",
    "                compteur_recherche+=1\n",
    "            #on récupère les hashtags, les urls et la longueur des tweets\n",
    "            for tweet in liste_tweets_utilisateur:\n",
    "                if tweet['urls'] != \"\" and liste_urls==\"\":\n",
    "                    liste_urls += tweet['urls']\n",
    "                elif tweet['urls'] != \"\" and liste_urls!=\"\":\n",
    "                    liste_urls += \", \" + tweet['urls']\n",
    "                if tweet['hashtags'] != \"\" and liste_hashtags==\"\":\n",
    "                    liste_hashtags += tweet['hashtags']\n",
    "                elif tweet['hashtags'] != \"\" and liste_hashtags!=\"\":\n",
    "                    liste_hashtags += \", \" + tweet['urls']\n",
    "                nombre_user_mentions+=int(tweet['user_mentions'])\n",
    "                total_text+=len(tweet['text'])\n",
    "            avg_text=total_text/len(liste_tweets_utilisateur)\n",
    "            nombre_urls = len(liste_urls.split(', '))\n",
    "            avg_urls=nombre_urls/len(liste_tweets_utilisateur)\n",
    "            #on calcule l'aggressivite\n",
    "            frequenceFriends=0\n",
    "            frequenceTweet=1\n",
    "            if len(liste_tweets_utilisateur)>1:\n",
    "                liste_timestamp=[]\n",
    "                for tweet in liste_tweets_utilisateur:\n",
    "                    liste_timestamp.append(tweet['timestamp'])\n",
    "                liste_timestamp.sort()\n",
    "                difftime=(liste_tweets_utilisateur[len(liste_tweets_utilisateur)-1]['timestamp']-liste_tweets_utilisateur[0]['timestamp'])/ float(3600000) + 0.000000000000001\n",
    "                frequenceTweet= len(liste_tweets_utilisateur)/ difftime\n",
    "                frequenceFriends = nombre_urls / difftime\n",
    "            agressivite = (frequenceFriends + frequenceTweet)/350\n",
    "            #on calcule la visibilté\n",
    "            moyLengthHashtags = 11.6\n",
    "            moyLengthMention = 11.4\n",
    "            nombre_hashtags = len(liste_hashtags.split(\",\"))\n",
    "            avg_hashtags = nombre_hashtags/len(liste_tweets_utilisateur)\n",
    "            avg_user_mentions = nombre_user_mentions / len(liste_tweets_utilisateur)\n",
    "            visibilite= ((avg_hashtags*moyLengthHashtags) + (avg_user_mentions*moyLengthMention))/140\n",
    "            #on incrémente le compteur\n",
    "            compteur+=1\n",
    "            #on stocke le user dans la db\n",
    "            final_user = ORM.User(id_user=rows['id_user'], followers_count=rows['followers_count'], friends_count=rows['friends_count'], ratio=indicateur_ratio, tweet_length=avg_text, nombre_moyen_url=avg_urls, nombre_moyen_hashtag=avg_hashtags, fav=rows['fav'], agressivite=agressivite, visibilite=visibilite)\n",
    "            ORM.session.add(final_user)\n",
    "            ORM.session.commit()\n",
    "            ORM.session.flush()\n",
    "            ORM.session.refresh(final_user)\n",
    "        if compteur % 1000 == 0:\n",
    "            print(compteur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On stocke ainsi les users dans notre base de données SQLite. On exporte ensuite la table des users en CSV pour l'utiliser dans notre projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données dans un DF Pandas\n",
    "On récupère les données parsées dans la base SQLite que l'on a exporté dans un .csv puis on enlève les colonnes identifiant et identifiant_user qui ne nous seront pas utiles pour notre analyse. On choisi de ne prendre que 400000 users pour nos traitements car nos machines ne supportent pas le traitement de toute la base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/finalDF.csv').drop([\"id\", \"id_user\"], axis=1).iloc[500000:900000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrage-réduction des données pour réaliser l'ACP\n",
    "On centre réduit les données pour supprimer la variabilité des données à cause de leur unité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On centre réduit les données\n",
    "s_sc = StandardScaler() \n",
    "df_processed = s_sc.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réalisation de l'ACP\n",
    "On réalise l'ACP pour réduire la dimensionnalité de notre dataframe, afin de réaliser l'algorithme non-supervisé K-Means. On passe donc de 8 variables à 2 variables qui sont les composantes principales retenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On réalise l'ACP\n",
    "modelPCA = PCA(n_components=2)\n",
    "df_reduced = modelPCA.fit_transform(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réaliser maintenant le cercle de corrélations, afin de déceler différentes corréltions entre les variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a variable factor map for the first two dimensions.\n",
    "(fig, ax) = plt.subplots(figsize=(8, 8))\n",
    "for i in range(0, modelPCA.components_.shape[1]):\n",
    "    ax.arrow(0,\n",
    "             0,  # Start the arrow at the origin\n",
    "             modelPCA.components_[0, i],  #0 for PC1\n",
    "             modelPCA.components_[1, i],  #1 for PC2\n",
    "             head_width=0.05,\n",
    "             head_length=0.05)\n",
    "\n",
    "    plt.text(modelPCA.components_[0, i] + 0.05,\n",
    "             modelPCA.components_[1, i] + 0.05,\n",
    "             df.columns.values[i])\n",
    "\n",
    "\n",
    "an = np.linspace(0, 2 * np.pi, 100)\n",
    "plt.plot(np.cos(an), np.sin(an))  # Add a unit circle for scale\n",
    "plt.axis('equal')\n",
    "ax.set_title('Variable factor map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère ici les variances expliquées par chaque composante principale afin d'avoir cette indication sur notre graph. On trace donc le graph avec les deux composantes principales comme axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP1inertie = str(round(round(modelPCA.explained_variance_ratio_[0],3)*100,1))\n",
    "CP2inertie = str(round(round(modelPCA.explained_variance_ratio_[1],3)*100,1))\n",
    "xlab = str(\"CP1 (\"+CP1inertie+\"%)\")\n",
    "ylab = str(\"CP2 (\"+CP2inertie+\"%)\")\n",
    "plt.scatter(df_reduced[:,0],df_reduced[:,1])\n",
    "plt.xlabel(xlab)\n",
    "plt.ylabel(ylab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouver le nombre de clusters pour le K-Means\n",
    "Pour trouver le nombre de clusters que nous allons prendre pour le K-Means, on réalise la méthode du coude représentant l'inertie en fonction du nombre de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow method\n",
    "inertia = []\n",
    "K_range = range(1, 8)\n",
    "for i in K_range:\n",
    "    modelElbow = KMeans(n_clusters=i).fit(df_reduced)\n",
    "    inertia.append(modelElbow.inertia_)\n",
    "\n",
    "plt.plot(K_range, inertia)\n",
    "plt.xlabel('nb de clusters')\n",
    "plt.ylabel('Inertie')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means avec 3 clusters\n",
    "On réalise le K-Means avec 3 clusters, car c'est notre zone de coude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans with 3 clusters\n",
    "modelKMeans = KMeans(n_clusters=3)\n",
    "df_KMeans = modelKMeans.fit(df_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trace le graph en donnant une couleur définie à nos clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = modelKMeans.fit_predict(df_reduced)\n",
    "\n",
    "filtered_label0 = df_reduced[label == 0]\n",
    "filtered_label1 = df_reduced[label == 1]\n",
    "filtered_label2 = df_reduced[label == 2]\n",
    "plt.scatter(filtered_label0[:,0] , filtered_label0[:,1] , color = 'red')\n",
    "plt.scatter(filtered_label1[:,0] , filtered_label1[:,1] , color = 'black')\n",
    "plt.scatter(filtered_label2[:,0] , filtered_label2[:,1] , color = 'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means avec 6 clusters\n",
    "Pour plus de représentativité par rapport à la problématique, on réalise un K-Means avec 6 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans with 6 clusters\n",
    "modelKMeans = KMeans(n_clusters=6)\n",
    "df_KMeans = modelKMeans.fit(df_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trace le graph avec les 6 clusters en choisissant la couleur de chacun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = modelKMeans.fit_predict(df_reduced)\n",
    "\n",
    "filtered_label0 = df_reduced[label == 0]\n",
    "filtered_label1 = df_reduced[label == 1]\n",
    "filtered_label2 = df_reduced[label == 2]\n",
    "filtered_label3 = df_reduced[label == 3]\n",
    "filtered_label4 = df_reduced[label == 4]\n",
    "filtered_label5 = df_reduced[label == 5]\n",
    "plt.scatter(filtered_label0[:,0] , filtered_label0[:,1] , color = 'red')\n",
    "plt.scatter(filtered_label1[:,0] , filtered_label1[:,1] , color = 'black')\n",
    "plt.scatter(filtered_label2[:,0] , filtered_label2[:,1] , color = 'green')\n",
    "plt.scatter(filtered_label3[:,0] , filtered_label3[:,1] , color = 'cyan')\n",
    "plt.scatter(filtered_label4[:,0] , filtered_label4[:,1] , color = 'magenta')\n",
    "plt.scatter(filtered_label5[:,0] , filtered_label5[:,1] , color = 'yellow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labellisation de nos utilisateurs\n",
    "On labellise les utilisateurs comme suspect ou non en fonction du résultat du K-Means. Le cluster ayant le pourcentage de comptes supprimés le plus élevé est considéré comme étant suspect. Le cluster que nous avons identifié est le plus en bas à gauche sur le graph du KMeans, il faut donc ajuster le résultat du graph KMeans à cette labellisation en prenant comme suspect la couleur du cluster le plus en bas à gauche sur le code ci-dessous (cluster0 : rouge ; cluster 1 : noir ; cluster 2 : vert ; cluster 3 : cyan ; cluster 4 : magenta ; cluster5 : jaune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User from each clusters\n",
    "\n",
    "cluster0 = pd.DataFrame(df_reduced[df_KMeans.labels_==0])\n",
    "cluster1 = pd.DataFrame(df_reduced[df_KMeans.labels_==1])\n",
    "cluster2 = pd.DataFrame(df_reduced[df_KMeans.labels_==2])\n",
    "cluster3 = pd.DataFrame(df_reduced[df_KMeans.labels_==3])\n",
    "cluster4 = pd.DataFrame(df_reduced[df_KMeans.labels_==4])\n",
    "cluster5 = pd.DataFrame(df_reduced[df_KMeans.labels_==5])\n",
    "\n",
    "cluster0['suspect'] = 0\n",
    "cluster1['suspect'] = 0\n",
    "cluster2['suspect'] = 0\n",
    "cluster3['suspect'] = 1\n",
    "cluster4['suspect'] = 0\n",
    "cluster5['suspect'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " On concatène nos 6 clusters pour creer un dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label = pd.concat([cluster0, cluster1, cluster2, cluster3, cluster4, cluster5])\n",
    "dataset_label.to_csv('data/dataset_label.csv', encoding='utf-8')\n",
    "dataset_final = np.array(dataset_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "## Séparation des labels\n",
    "On sépare les variables X de leur label Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_final[:,:-1]\n",
    "Y = dataset_final[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répartition Training/Test\n",
    "On sépare notre jeu de données en 2 sous-datasets, un pour l'entrainement du model et un pour le test, comprenant respectivement 80% et 20% des données prises aléatoirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du model\n",
    "On entraine tout d'abord le model avec la fonction noyau 'linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = svm.SVC(kernel='linear')\n",
    "\n",
    "linear.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test et analyse\n",
    "On test le model sur nos données test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On analyse les résultats avec le dataset de test et on créé le classification report pour la fonction linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))\n",
    "pd.DataFrame(classification_report(Y_test,Y_pred, output_dict = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], c = Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Matrice de confusion, avec normalisation\", None),\n",
    "                  (\"Matrice de confusion normalisée\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(linear, X_test, Y_test,\n",
    "                                 display_labels=['normal','suspect'],\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(classification_report(Y_test,Y_pred, output_dict = True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM avec la fonction poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = svm.SVC(kernel='poly')\n",
    "\n",
    "poly.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = poly.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], c = Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(classification_report(Y_test,Y_pred, output_dict = True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM avec la fonction rbf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = svm.SVC(kernel='rbf')\n",
    "\n",
    "rbf.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c = Y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], c = Y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(classification_report(Y_test,Y_pred, output_dict = True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b893e444e619e70b2cf399b639be411f8879b6bd72449a06e4d1173d8380f2c2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}